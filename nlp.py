# -*- coding: utf-8 -*-
"""hackathon.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xkwxyF5eovsfBvFx1ju6Dfb-KavK3hSA
"""

import pandas as pd 
import random
import numpy as np

dataset = pd.read_csv("test (1).csv")
row,cols = dataset.shape
print(row)

random_weights = []
for i in range(0,row):
    random_weights.append(int(random.sample(range(1,100),1)[0]))

dataset["Weights"] = random_weights
dataset = dataset.sort_values("Weights", ascending=False)
print(dataset["Weights"].max())

final = (dataset['Text Content'])
print(final)

!pip install transformers
!pip install pytorch

from transformers import pipeline
classifier = pipeline("zero-shot-classification",model="facebook/bart-large-mnli")

sequence_to_classify = "soham kulkarni"
array_final = sequence_to_classify.split(':')
print(array_final)
# array_final = array_final.split()
# print(array_final)
print(array_final[0])
s = array_final[0]
s = s.split()

candidate_labels = ['name', 'price', 'product', 'address', 'phone number']
s = np.concatenate((s, candidate_labels))
print(s)

print(classifier(sequence_to_classify, s))

import spacy

nlp = spacy.load("en_core_web_sm")

text = "Terms and Conditions Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua."

doc = nlp(text)

for token in doc:
    if token.text == "Terms" and token.nbor().text == "and" and token.nbor(2).text == "Conditions":
        if "Terms" in doc.text and "And" in doc.text or "Conditions" in doc.text and "And" in doc.text or "Conditions" in doc.text and "Term" in doc.text:
            print("The 'Terms and Conditions' are important.")
            break
        else:
            print("The 'Terms and Conditions' are not explicitly marked as important.")
            break

review = "Terms and Conditions Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua."
import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
corpus = []

review = review.lower()
review = review.split()
ps = PorterStemmer()
all_stopwords = stopwords.words('english')
all_stopwords.remove('not')
review = [ps.stem(word) for word in review if not word in set(all_stopwords)]
review = ' '.join(review)
corpus.append(review)

print(corpus)